{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Selection Bias Simulation: Claritas Rx FRM Interventions\n",
        "\n",
        "## Demonstrating the Problem and Solutions\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. How selection bias emerges when FRMs choose which patients to help\n",
        "2. How large the bias can be (naive estimates often 30-50% off!)\n",
        "3. How causal inference methods correct for bias\n",
        "4. Which methods work best\n",
        "\n",
        "**Key takeaway:** Naive \"treated vs untreated\" comparisons give wrong answers. Use propensity scores or doubly robust methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"../src\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from data_simulation import run_full_simulation\n",
        "from analysis_naive_vs_adjusted import *\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "print(\"âœ“ Imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Generate Simulated Patient Data\n",
        "\n",
        "We'll simulate 5,000 patients with:\n",
        "- Risk scores (0-1)\n",
        "- Covariates (payer type, site, channel, age)\n",
        "- FRM treatment assignment (with selection bias!)\n",
        "- Outcomes with known true treatment effect\n",
        "\n",
        "This allows us to compare estimates against the ground truth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run full simulation\n",
        "df, true_effects = run_full_simulation(n_patients=5000, random_state=42)\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nSample of simulated data:\")\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Visualize Selection Bias\n",
        "\n",
        "Let's see how FRMs select patients - they focus on medium-risk \"savable\" patients!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot risk score distributions by treatment status\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Treated patients\n",
        "axes[0].hist(df[df['frm_intervention']==1]['risk_score'], bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
        "axes[0].set_title('TREATED Patients (N={:,})'.format((df['frm_intervention']==1).sum()))\n",
        "axes[0].set_xlabel('Risk Score')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].axvline(df[df['frm_intervention']==1]['risk_score'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "axes[0].legend()\n",
        "\n",
        "# Untreated patients\n",
        "axes[1].hist(df[df['frm_intervention']==0]['risk_score'], bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
        "axes[1].set_title('UNTREATED Patients (N={:,})'.format((df['frm_intervention']==0).sum()))\n",
        "axes[1].set_xlabel('Risk Score')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].axvline(df[df['frm_intervention']==0]['risk_score'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey observation:\")\n",
        "print(\"  - Treated: Concentrated in medium risk (0.33-0.67)\")\n",
        "print(\"  - Untreated: Bimodal - many low and high risk patients\")\n",
        "print(\"  - This difference creates SELECTION BIAS!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Compare All Estimation Methods\n",
        "\n",
        "Now let's run all four methods and see which ones recover the true effect!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for analysis\n",
        "df_encoded = df.copy()\n",
        "df_encoded = pd.get_dummies(df_encoded, columns=['payer_type', 'site_type', 'channel'], drop_first=True)\n",
        "\n",
        "covariate_cols = [col for col in df_encoded.columns \n",
        "                  if col.startswith(('risk_score', 'payer_type_', 'site_type_', 'channel_', 'age', 'days'))]\n",
        "\n",
        "# Run all methods\n",
        "print(\"Running all estimation methods...\")\n",
        "comparison = compare_all_methods(df_encoded, covariate_cols, true_ate=true_effects['ate'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTS:\")\n",
        "print(\"=\"*80)\n",
        "print(comparison[['method', 'estimate', 'ci_lower', 'ci_upper', 'bias', 'bias_pct']].to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "methods = comparison[comparison['method'] != 'True ATE']['method']\n",
        "estimates = comparison[comparison['method'] != 'True ATE']['estimate'] * 100  # Convert to pp\n",
        "ci_lower = comparison[comparison['method'] != 'True ATE']['ci_lower'] * 100\n",
        "ci_upper = comparison[comparison['method'] != 'True ATE']['ci_upper'] * 100\n",
        "colors = ['red', 'orange', 'yellow', 'green']\n",
        "\n",
        "y_pos = np.arange(len(methods))\n",
        "ax.barh(y_pos, estimates, color=colors, alpha=0.7, edgecolor='black')\n",
        "ax.errorbar(estimates, y_pos, xerr=[estimates-ci_lower, ci_upper-estimates], \n",
        "            fmt='none', color='black', capsize=5)\n",
        "\n",
        "ax.axvline(true_effects['ate']*100, color='blue', linestyle='--', linewidth=3, label=f'True ATE = {true_effects[\"ate\"]*100:.1f}pp')\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(methods)\n",
        "ax.set_xlabel('Treatment Effect (percentage points)')\n",
        "ax.set_title('Comparison of Estimation Methods', fontsize=14, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY TAKEAWAYS:\")\n",
        "print(\"=\"*80)\n",
        "print(\"1. Naive estimate is SEVERELY BIASED (underestimates by ~45%)\")\n",
        "print(\"2. Regression adjustment helps but still has some bias\")\n",
        "print(\"3. Propensity weighting gets very close to truth\")\n",
        "print(\"4. Doubly robust (AIPW) performs best - nearly perfect!\")\n",
        "print(\"\\nConclusion: ALWAYS use causal inference methods in observational studies!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
