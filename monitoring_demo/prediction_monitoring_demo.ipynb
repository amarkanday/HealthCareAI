{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Output Monitoring Demo\n",
        "## Prediction Drift and Performance Detection\n",
        "\n",
        "This notebook demonstrates model output monitoring including:\n",
        "- Prediction distribution drift detection\n",
        "- Recommendation rate stability\n",
        "- Model output anomalies\n",
        "- Performance monitoring (when outcomes available)\n",
        "\n",
        "**Use Case:** Monitor Random Forest uplift model predictions for the Intervention Recommendation Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from prediction_monitor import PredictionMonitor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"\u2713 Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Baseline and Daily Predictions\n",
        "\n",
        "Predictions from Random Forest T-Learner model (two models for uplift estimation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load baseline predictions (first 14 days)\n",
        "baseline_predictions = pd.read_parquet('./data/baseline_predictions.parquet')\n",
        "\n",
        "print(f\"Baseline predictions: {len(baseline_predictions):,} rows\")\n",
        "print(f\"\\nColumns:\")\n",
        "print(list(baseline_predictions.columns))\n",
        "print(f\"\\nSummary statistics:\")\n",
        "baseline_predictions[['model_treated_prob', 'model_control_prob', 'uplift_score', 'recommendation']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all 30 days of predictions\n",
        "import glob\n",
        "prediction_files = sorted(glob.glob('./data/predictions_*.parquet'))\n",
        "\n",
        "daily_predictions = []\n",
        "for file in prediction_files:\n",
        "    df = pd.read_parquet(file)\n",
        "    daily_predictions.append(df)\n",
        "\n",
        "print(f\"\u2713 Loaded {len(daily_predictions)} days of predictions\")\n",
        "print(f\"  - Each day: ~{len(daily_predictions[0]):,} predictions\")\n",
        "print(f\"\\nSample from Day 1:\")\n",
        "daily_predictions[0].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Prediction Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize monitor with baseline\n",
        "pred_monitor = PredictionMonitor(baseline_predictions)\n",
        "\n",
        "print(f\"\u2713 Prediction monitor initialized\")\n",
        "print(f\"\\nBaseline statistics:\")\n",
        "print(json.dumps(pred_monitor.baseline_stats, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run Daily Prediction Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run monitoring for all 30 days\n",
        "pred_monitoring_results = []\n",
        "\n",
        "for day_idx, df in enumerate(daily_predictions):\n",
        "    result = pred_monitor.monitor_daily_predictions(df)\n",
        "    result['day'] = day_idx + 1\n",
        "    pred_monitoring_results.append(result)\n",
        "\n",
        "print(f\"\u2713 Prediction monitoring completed for {len(pred_monitoring_results)} days\")\n",
        "print(f\"\\nDaily Summary:\")\n",
        "print(\"Day | Status | Drift | Alerts | Rec Rate\")\n",
        "print(\"-\" * 50)\n",
        "for result in pred_monitoring_results:\n",
        "    summary = result['summary']\n",
        "    stats = result['current_stats']\n",
        "    drift = result['drift_test']['drifted']\n",
        "    status = '\ud83d\udd34' if summary['high_severity_alerts'] > 0 else '\ud83d\udfe1' if summary['total_alerts'] > 0 else '\ud83d\udfe2'\n",
        "    drift_icon = '\u26a0\ufe0f' if drift else '\u2713'\n",
        "    print(f\" {result['day']:2d} |   {status}    |  {drift_icon}   |   {summary['total_alerts']:2d}   | {stats['recommendation_rate']*100:5.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualize Prediction Monitoring Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract time series\n",
        "days = [r['day'] for r in pred_monitoring_results]\n",
        "mean_uplift = [r['current_stats']['mean_uplift'] for r in pred_monitoring_results]\n",
        "rec_rate = [r['current_stats']['recommendation_rate'] * 100 for r in pred_monitoring_results]\n",
        "ks_pvalues = [r['drift_test']['ks_p_value'] for r in pred_monitoring_results]\n",
        "alerts = [r['summary']['total_alerts'] for r in pred_monitoring_results]\n",
        "\n",
        "# Create dashboard\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Prediction Monitoring Dashboard - 30 Days', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Mean uplift score over time\n",
        "baseline_mean = pred_monitor.baseline_stats['mean_uplift']\n",
        "axes[0, 0].plot(days, mean_uplift, marker='o', color='blue', linewidth=2, label='Current')\n",
        "axes[0, 0].axhline(y=baseline_mean, color='green', linestyle='--', linewidth=2, label='Baseline')\n",
        "axes[0, 0].set_xlabel('Day')\n",
        "axes[0, 0].set_ylabel('Mean Uplift Score')\n",
        "axes[0, 0].set_title('Mean Uplift Score Over Time')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Recommendation rate over time\n",
        "baseline_rec_rate = pred_monitor.baseline_stats['recommendation_rate'] * 100\n",
        "axes[0, 1].plot(days, rec_rate, marker='s', color='orange', linewidth=2, label='Current')\n",
        "axes[0, 1].axhline(y=baseline_rec_rate, color='green', linestyle='--', linewidth=2, label='Baseline')\n",
        "axes[0, 1].axhline(y=baseline_rec_rate + 10, color='red', linestyle=':', alpha=0.5, label='Alert threshold')\n",
        "axes[0, 1].axhline(y=baseline_rec_rate - 10, color='red', linestyle=':', alpha=0.5)\n",
        "axes[0, 1].set_xlabel('Day')\n",
        "axes[0, 1].set_ylabel('Recommendation Rate (%)')\n",
        "axes[0, 1].set_title('Patients Recommended for Intervention')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Distribution drift (KS test p-value)\n",
        "axes[1, 0].plot(days, ks_pvalues, marker='o', color='purple', linewidth=2)\n",
        "axes[1, 0].axhline(y=0.01, color='red', linestyle='--', label='Significance threshold (p=0.01)')\n",
        "axes[1, 0].set_xlabel('Day')\n",
        "axes[1, 0].set_ylabel('KS Test p-value')\n",
        "axes[1, 0].set_title('Uplift Score Distribution Drift')\n",
        "axes[1, 0].set_yscale('log')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Alerts over time\n",
        "axes[1, 1].bar(days, alerts, color='red', alpha=0.7)\n",
        "axes[1, 1].set_xlabel('Day')\n",
        "axes[1, 1].set_ylabel('Alert Count')\n",
        "axes[1, 1].set_title('Daily Alerts')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./prediction_monitoring_dashboard.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\u2713 Visualizations saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Uplift Score Distribution Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare uplift distributions: baseline vs current\n",
        "day_1 = daily_predictions[0]\n",
        "day_15 = daily_predictions[14]\n",
        "day_30 = daily_predictions[29]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "fig.suptitle('Uplift Score Distribution Over Time', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Day 1\n",
        "axes[0].hist(day_1['uplift_score'], bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
        "axes[0].axvline(day_1['uplift_score'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {day_1[\"uplift_score\"].mean():.3f}')\n",
        "axes[0].set_xlabel('Uplift Score')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Day 1 (Baseline)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Day 15\n",
        "axes[1].hist(day_15['uplift_score'], bins=50, color='orange', alpha=0.7, edgecolor='black')\n",
        "axes[1].axvline(day_15['uplift_score'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {day_15[\"uplift_score\"].mean():.3f}')\n",
        "axes[1].set_xlabel('Uplift Score')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Day 15')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Day 30\n",
        "axes[2].hist(day_30['uplift_score'], bins=50, color='green', alpha=0.7, edgecolor='black')\n",
        "axes[2].axvline(day_30['uplift_score'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {day_30[\"uplift_score\"].mean():.3f}')\n",
        "axes[2].set_xlabel('Uplift Score')\n",
        "axes[2].set_ylabel('Frequency')\n",
        "axes[2].set_title('Day 30')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./uplift_distribution_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Alert Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract all prediction alerts\n",
        "pred_alerts = []\n",
        "for result in pred_monitoring_results:\n",
        "    for alert in result['alerts']:\n",
        "        pred_alerts.append({\n",
        "            'day': result['day'],\n",
        "            'severity': alert['severity'],\n",
        "            'metric': alert['metric'],\n",
        "            'message': alert['message']\n",
        "        })\n",
        "\n",
        "if len(pred_alerts) > 0:\n",
        "    pred_alerts_df = pd.DataFrame(pred_alerts)\n",
        "    \n",
        "    print(f\"Total prediction alerts: {len(pred_alerts_df)}\")\n",
        "    print(f\"\\nAlerts by severity:\")\n",
        "    print(pred_alerts_df['severity'].value_counts())\n",
        "    print(f\"\\nAlerts by metric:\")\n",
        "    print(pred_alerts_df['metric'].value_counts())\n",
        "    \n",
        "    if len(pred_alerts_df[pred_alerts_df['severity'] == 'HIGH']) > 0:\n",
        "        print(f\"\\n\" + \"=\"*80)\n",
        "        print(\"HIGH SEVERITY ALERTS:\")\n",
        "        print(\"=\"*80)\n",
        "        high_alerts = pred_alerts_df[pred_alerts_df['severity'] == 'HIGH']\n",
        "        for idx, row in high_alerts.iterrows():\n",
        "            print(f\"Day {row['day']:2d} | {row['metric']:25s} | {row['message']}\")\n",
        "else:\n",
        "    print(\"\u2713 No alerts detected - predictions are stable!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary & Key Findings\n",
        "\n",
        "### Detection Capabilities:\n",
        "\n",
        "\u2705 **Successfully Monitors:**\n",
        "- Uplift score distribution stability (KS test)\n",
        "- Mean uplift score shifts (z-score)\n",
        "- Recommendation rate changes\n",
        "- Extreme predictions (scores <0 or >1)\n",
        "- Model component drift (treated/control model outputs)\n",
        "\n",
        "### Production Recommendations:\n",
        "\n",
        "1. **Daily Monitoring:** Run after daily scoring completes\n",
        "2. **Alerting:** \n",
        "   - HIGH: Mean shift >3 std devs OR extreme predictions >1%\n",
        "   - MEDIUM: Distribution drift OR rec rate change >10pp\n",
        "3. **Model Refresh:** Consider retraining if alerts persist >5 days\n",
        "4. **Root Cause:** Check input data drift first (likely upstream cause)\n",
        "5. **Model Health:** Random Forest models are typically stable - alerts indicate data issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\"*80)\n",
        "print(\"PREDICTION MONITORING SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nMonitoring Period: 30 days\")\n",
        "print(f\"Baseline: Days 1-14\")\n",
        "print(f\"Model: Random Forest T-Learner (Uplift)\")\n",
        "print(f\"\\nPrediction Statistics:\")\n",
        "print(f\"  - Total predictions: {sum(len(d) for d in daily_predictions):,}\")\n",
        "print(f\"  - Avg daily predictions: {np.mean([len(d) for d in daily_predictions]):.0f}\")\n",
        "print(f\"  - Avg recommendation rate: {np.mean(rec_rate):.1f}%\")\n",
        "print(f\"\\nMonitoring Results:\")\n",
        "if len(pred_alerts) > 0:\n",
        "    pred_alerts_df = pd.DataFrame(pred_alerts)\n",
        "    print(f\"  - Total alerts: {len(pred_alerts_df)}\")\n",
        "    print(f\"  - HIGH severity: {len(pred_alerts_df[pred_alerts_df['severity'] == 'HIGH'])}\")\n",
        "    print(f\"  - MEDIUM severity: {len(pred_alerts_df[pred_alerts_df['severity'] == 'MEDIUM'])}\")\n",
        "else:\n",
        "    print(f\"  - Total alerts: 0\")\n",
        "print(f\"  - Days with drift: {sum(1 for r in pred_monitoring_results if r['drift_test']['drifted'])}\")\n",
        "print(f\"\\nModel Stability: {'\ud83d\udfe2 STABLE' if len(pred_alerts) == 0 else '\ud83d\udfe1 MONITOR' if all(a['severity'] != 'HIGH' for a in pred_alerts) else '\ud83d\udd34 ACTION NEEDED'}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\u2713 Prediction monitoring demo complete!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}